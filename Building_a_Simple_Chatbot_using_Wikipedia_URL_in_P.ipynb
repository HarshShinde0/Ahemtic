{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Building a Simple Chatbot using Wikipedia URL in P",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshShinde0/Ahemtic/blob/main/Building_a_Simple_Chatbot_using_Wikipedia_URL_in_P.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Chatbot using Wikipedia URL in Python(using NLTK) With Voice Assistance"
      ],
      "metadata": {
        "id": "DRW9IEhDR5Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project By Harsh Shinde\n",
        "Reference - Parul Pandey"
      ],
      "metadata": {
        "id": "posJTvtfR5Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP\n",
        "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
      ],
      "metadata": {
        "id": "1vF9iCxBR5Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "trusted": true,
        "id": "-yOPKoieR5Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper3k"
      ],
      "metadata": {
        "trusted": true,
        "id": "vgma971WR5Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyttsx3"
      ],
      "metadata": {
        "trusted": true,
        "id": "Bzt0UCgOR5Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from newspaper import Article\n",
        "import pyttsx3"
      ],
      "metadata": {
        "trusted": true,
        "id": "JIpU6YyFR5Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and installing NLTK\n",
        "\n",
        "NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
        "\n",
        "[Natural Language Processing with Python](http://www.nltk.org/book/) provides a practical introduction to programming for language processing.\n",
        "\n",
        "For platform-specific instructions, read [here](https://www.nltk.org/install.html)"
      ],
      "metadata": {
        "id": "4uHZn4CuR5Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text-to-speech engine\n",
        "engine = pyttsx3.init()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0YsAku05R5Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download package from nltk\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('popular', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BnPX-ZeOR5Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to normalize text\n",
        "def LemNormalize(text):\n",
        "    return nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "IdYlwXwkR5Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords for greetings\n",
        "greeting_input = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\"]\n",
        "greeting_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "w45JaZQER5Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recognize greetings\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greeting_input:\n",
        "            return random.choice(greeting_response)"
      ],
      "metadata": {
        "trusted": true,
        "id": "g3hypulwR5Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate responses\n",
        "def response(user_response):\n",
        "    user_response = user_response.lower()\n",
        "    user_response = user_response.upper()\n",
        "    robo_response = ''\n",
        "    sent_tokens.append(user_response)\n",
        "    tfidfvec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = tfidfvec.fit_transform(sent_tokens)\n",
        "    val = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx = val.argsort()[0][-2]\n",
        "    flat = val.flatten()\n",
        "    flat.sort()\n",
        "    score = flat[-2]\n",
        "    if score == 0:\n",
        "        robo_response = \"Sorry, I don't understand.\"\n",
        "    else:\n",
        "        robo_response = sent_tokens[idx]\n",
        "\n",
        "    sent_tokens.remove(user_response)\n",
        "    speak(robo_response)  # Speak the response\n",
        "    return robo_response"
      ],
      "metadata": {
        "trusted": true,
        "id": "CYQcR1DRR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to speak text\n",
        "def speak(text):\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()"
      ],
      "metadata": {
        "trusted": true,
        "id": "01BCwbEjR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get article URL\n",
        "article = Article('https://simple.wikipedia.org/wiki/Earth')\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "corpus = article.text\n",
        "#print\n",
        "print(corpus)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OpdfzjKKR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "text = corpus\n",
        "sent_tokens = nltk.sent_tokenize(text)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OOIC-tqkR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "kqIHgQvwR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dictionary to remove the punctuation\n",
        "print(string.punctuation)\n",
        "print(remove_punct_dict)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_1SPwnY0R5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a function to return lower case words\n",
        "def LemNormalize(text):\n",
        "  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "print(LemNormalize(text))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ez3H7YBcR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keywords for greetings\n",
        "greeting_input=[\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\"]\n",
        "greeting_response=[\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greeting_input:\n",
        "      return random.choice(greeting_response)"
      ],
      "metadata": {
        "trusted": true,
        "id": "f4XgmZnKR5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary to remove punctuation\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "flag = True\n",
        "print(\"Hello! This is HKS Bot. I can answer your queries related to anything. Just provide a Wikipedia URL. Type 'bye' to exit.\")\n",
        "while flag:\n",
        "    user_response = input(\"\\nUSER: \")\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thanks' or user_response == 'thank you':\n",
        "            flag = False\n",
        "            print(\"HKS BOT: Anytime! :)\")\n",
        "        else:\n",
        "            if greeting(user_response) is not None:\n",
        "                print(\"HKS BOT: \" + greeting(user_response))\n",
        "            else:\n",
        "                print(\"HKS BOT: \" + response(user_response))\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"HKS BOT: See you later! :)\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "pujKBhOiR5Mk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}